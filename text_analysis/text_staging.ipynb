{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "dank_data = pd.read_csv(\"./data_ocr.csv\", delimiter = \",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dank_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dank_data.insert(4, \"words\", \"Empty\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind in dank_data.index:    \n",
    "    if dank_data['words'][ind] == 'Empty':\n",
    "        title = dank_data['title'][ind]\n",
    "        text_from_image = dank_data['text_from_image'][ind]\n",
    "        try:\n",
    "            if text_from_image == 'Fail':\n",
    "                text_from_image = ''  \n",
    "            words = title +  text_from_image\n",
    "        except:\n",
    "            words = 'Fail'\n",
    "        dank_data['words'][ind] = words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = 0\n",
    "for ind in dank_data.index:     \n",
    "\n",
    "    if (dank_data['words'][ind] == 'Fail'):\n",
    "        result = result + 1\n",
    "\n",
    "print(\" result = \", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# row in which value of 'text_from_image' column == \"Fail\"\n",
    "failed_link_memes = dank_data.apply(lambda x: True if x['words'] == 'Fail' else False , axis=1)\n",
    " \n",
    "# Count number of True in series\n",
    "numOfRows = len(failed_link_memes[failed_link_memes == True].index)\n",
    " \n",
    "print('Number of Rows in dataframe with broken link : ', numOfRows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dank_data.drop(dank_data.index[dank_data['words'] == 'Fail'], inplace = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# row in which value of 'text_from_image' column == \"Fail\"\n",
    "failed_link_memes = dank_data.apply(lambda x: True if x['words'] == 'Fail' else False , axis=1)\n",
    " \n",
    "# Count number of True in series\n",
    "numOfRows = len(failed_link_memes[failed_link_memes == True].index)\n",
    " \n",
    "print('Number of Rows in dataframe with broken link : ', numOfRows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # remove wrong convertion characters from title\n",
    "    text = text.map(lambda x: re.sub(r'\\\\x[00-ff]{2}','',str(x)))\n",
    "    text = text.str.strip()\n",
    "    text = text.map(lambda x: re.sub('b\"','',str(x)))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "dank_data['words'] = clean_text(dank_data['words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dank_data['ups_normed'] = dank_data['ups']/dank_data['subscribers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate length of the titles\n",
    "dank_data['TextLength'] = dank_data['words'].str.len()\n",
    "dank_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dank_data.drop(columns=['text_from_image' ], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Bidirectional\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Dropout\n",
    "from keras.models import model_from_json\n",
    "from keras.models import load_model\n",
    "import h5py\n",
    "from keras.models import model_from_json\n",
    "from keras.models import load_model\n",
    "from flask import Flask, url_for, request\n",
    "import json\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load saved LSTM model\n",
    "from numpy import loadtxt\n",
    "from keras.models import load_model\n",
    " \n",
    "# load model\n",
    "LSTM_model = load_model('model.hdf5')\n",
    "# summarize model.\n",
    "LSTM_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def live_analyses(trained_model, data, word_idx):\n",
    "    live_list = []\n",
    "    live_list_np = np.zeros((56,1))\n",
    "    # split the sentence into its words and remove any punctuations.\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    data_sample_list = tokenizer.tokenize(data)\n",
    "    labels = np.array(['1','2','3','4','5','6','7','8','9','10'], dtype = \"int\")\n",
    "    # get index for the live stage\n",
    "    data_index = np.array([word_idx[word.lower()] if word.lower() in word_idx else 0 for word in data_sample_list])\n",
    "    data_index_np = np.array(data_index)\n",
    "    # padded with zeros of length 56 i.e maximum length\n",
    "    padded_array = np.zeros(56)\n",
    "    padded_array[:data_index_np.shape[0]] = data_index_np\n",
    "    data_index_np_pad = padded_array.astype(int)\n",
    "    live_list.append(data_index_np_pad)\n",
    "    live_list_np = np.asarray(live_list)\n",
    "    # get score from the model\n",
    "    score = trained_model.predict(live_list_np, batch_size=1, verbose=0)\n",
    "    single_score = np.round(np.argmax(score)/10, decimals=2) # maximum of the array i.e single band\n",
    "    # weighted score of top 3 bands\n",
    "    top_3_index = np.argsort(score)[0][-3:]\n",
    "    top_3_scores = score[0][top_3_index]\n",
    "    top_3_weights = top_3_scores/np.sum(top_3_scores)\n",
    "    single_score_dot = np.round(np.dot(top_3_index, top_3_weights)/10, decimals = 2)\n",
    "    return single_score_dot, single_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_idx = json.load(open(\"Data_word_idx.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source https://towardsdatascience.com/nlp-extracting-the-main-topics-from-your-dataset-using-lda-in-minutes-21486f5aa925\n",
    "'''\n",
    "Loading Gensim and nltk libraries\n",
    "'''\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dank_data.insert(5, \"processed_words\", \"Empty\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Write a function to perform the pre processing steps on the entire dataset\n",
    "'''\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "# Tokenize and lemmatize\n",
    "def preprocess(text):\n",
    "    result=[]\n",
    "    for token in gensim.utils.simple_preprocess(text) :\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "  \n",
    "for ind in dank_data.index:     \n",
    "    if dank_data['processed_words'][ind] == 'Empty':\n",
    "        words = dank_data['words'][ind]\n",
    "        try:\n",
    "            processed_words = preprocess(words)\n",
    "        except:\n",
    "            processed_words = 'Fail'\n",
    "        dank_data['processed_words'][ind] = processed_words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dank_data.drop(columns=['Sentiment' ], axis=1, inplace=True)\n",
    "#dank_data.drop(columns=['Text' ], axis=1, inplace=True)\n",
    "dank_data['Text'] = 'Empty'\n",
    "dank_data['Sentiment'] = 0.0\n",
    "for ind in dank_data.index:\n",
    "    list_word = dank_data['processed_words'][ind]\n",
    "    MemeStr = ' '.join(list_word)\n",
    "    dank_data['Text'][ind] = MemeStr\n",
    "    MemeStr = MemeStr[:0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dank_data.drop(columns=['words'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dank_data.drop(columns=['Sentiment' ], axis=1, inplace=True)\n",
    "#dank_data['Sentiment'] = 0.0\n",
    "for ind in dank_data.index:\n",
    "    text = dank_data['Text'][ind][:250]\n",
    "    result = np.array(live_analyses(LSTM_model,text, word_idx))[0]\n",
    "    dank_data['Sentiment'][ind] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dank_data.drop(columns=['Text'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dank_data['word_count'] = 0\n",
    "for ind in dank_data.index:\n",
    "  dank_data['word_count'][ind] = len(dank_data['processed_words'][ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dank_data.to_json(r'./dank_data_final.json')\n",
    "dank_data.to_csv('dank_data_final.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
